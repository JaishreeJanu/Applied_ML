{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64587731",
   "metadata": {},
   "source": [
    "## Exercise 0 : Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c373335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jaisu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8a181",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a784f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['sci.med', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63169923",
   "metadata": {},
   "source": [
    "#### Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fd654c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vocabulary\n",
    "vocabulary={}\n",
    "ind = 0\n",
    "\n",
    "def preprocess_text(text_data):\n",
    "    \"\"\"\n",
    "    Removes punctuation marks, special characters, lower case text, removes stop words and creates list of words\n",
    "    Returns: list of words\n",
    "    \"\"\"\n",
    "    text_data = text_data.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    text_data = text_data.replace('\\n', ' ')\n",
    "\n",
    "    text_list = text_data.split(' ')\n",
    "    text_list = list(map(lambda x: x.lower(), text_list))\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    text_words = [word for word in text_list if word not in stop_words and word!='']\n",
    "    \n",
    "    global ind\n",
    "    # Insert items in dictionary and put index as the value\n",
    "    for word in text_words:\n",
    "        if word not in vocabulary.keys():\n",
    "            vocabulary[word] = ind\n",
    "            ind += 1\n",
    "            \n",
    "    return text_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e915fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_items = []\n",
    "for news in train.data:\n",
    "    news_items.append(preprocess_text(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804bb16e",
   "metadata": {},
   "source": [
    "#### Bag-of-words feature representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13c61c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(news_item):\n",
    "    \"\"\"\n",
    "    Returns vector representation of the news_item,\n",
    "    where vector contains frequency of each unique item in vocabulary.\n",
    "    \"\"\"\n",
    "    count_words = {}\n",
    "    news_vec = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for word in news_item:\n",
    "        if word not in count_words.keys():\n",
    "            count_words[word] = 1\n",
    "        else:\n",
    "            count_words[word] += 1 \n",
    "    \n",
    "    for word in news_item:\n",
    "        news_vec[vocabulary[word]] = count_words[word]\n",
    "        \n",
    "    return news_vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d58e25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vecs = []\n",
    "for news in news_items:\n",
    "    news_vecs.append(bag_of_words(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a4121",
   "metadata": {},
   "source": [
    "#### TF-IDF feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7352b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(news_vecs):\n",
    "    \"\"\"\n",
    "    Returns idf dictionary for the available news corpus\n",
    "    \"\"\"\n",
    "    # For every unique word in vocabulary, stores its IDF value\n",
    "    idf_vec = {}\n",
    "    N = len(vocabulary)\n",
    "    for key, item in vocabulary.items():\n",
    "        count_docs = 0\n",
    "        # count news_items which contain the word\n",
    "        for news in news_vecs:\n",
    "            if news[item] != 0:\n",
    "                count_docs += 1\n",
    "        # Store the IDF value for that word(key)\n",
    "        idf_vec[key] = math.log10(N/count_docs)\n",
    "    \n",
    "    return idf_vec\n",
    "\n",
    "def tf_idf(news_vecs):\n",
    "    \"\"\"\n",
    "    Returns tf-idf for the news item\n",
    "    \"\"\"\n",
    "    \n",
    "    idf_vec = idf(news_vecs)\n",
    "    \n",
    "    for i, news_item in enumerate(news_vecs):\n",
    "        indexes = list(news_item.nonzero()[0])\n",
    "        total_words_in_doc = len(news_items[i])\n",
    "        for index in indexes:\n",
    "            key = list(vocabulary.keys())[index]\n",
    "            # Put tf-idf value for very news_item word\n",
    "            news_item[index] = (news_item[index]/total_words_in_doc)/idf_vec[key]\n",
    "        news_vecs[i] = news_item\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e003054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver code for TF-IDF\n",
    "tf_idf(news_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8336a",
   "metadata": {},
   "source": [
    "#### Splitting data into train, test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f12c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.data, train.target, test_size=0.2, random_state=1)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a93824",
   "metadata": {},
   "source": [
    "## Exercise 1 : Implementation of Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41cb14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian_NB:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "                \n",
    "    def train(self,news,labels):\n",
    "        \"\"\"\n",
    "        Finds prior, likelihood and evidence.\n",
    "        These values are used in test function for computing posterior probability\n",
    "        \"\"\"\n",
    "        # Computing probabilities of the classes, called prior belief\n",
    "        labels_0 = [item for item in labels if item==0]\n",
    "        self.probs_0 = len(labels_0)/len(labels)\n",
    "        \n",
    "        labels_1 = [item for item in labels if item==1]\n",
    "        self.probs_1 = len(labels_1)/len(labels)\n",
    "        \n",
    "        ## Separate the news items of each class\n",
    "        self.news_0 = []\n",
    "        self.news_1 = []\n",
    "        \n",
    "        for i,news_item in enumerate(news):\n",
    "            if labels[i] == 0:\n",
    "                self.news_0.append(news_item)\n",
    "            else:\n",
    "                self.news_1.append(news_item)\n",
    "        \n",
    "        ## Now finding probability of each word given a target class\n",
    "        ## For class 0\n",
    "        self.feature_probs_0 = {} ## Will be of length vocabulary, i.e. for each word\n",
    "        for key, value in vocabulary.items():\n",
    "            count_news = 0\n",
    "            total = len(self.news_0)\n",
    "            for news_item in self.news_0:\n",
    "                if news_item[value] != 0:\n",
    "                    count_news += 1\n",
    "            self.feature_probs_0[key] = count_news/total\n",
    "            \n",
    "        \n",
    "        ## For class 1\n",
    "        self.feature_probs_1 = {} ## Will be of length vocabulary, i.e. for each word\n",
    "        for key, value in vocabulary.items():\n",
    "            count_news = 0\n",
    "            total = len(self.news_1)\n",
    "            for news_item in self.news_1:\n",
    "                if news_item[value] != 0:\n",
    "                    count_news += 1\n",
    "            self.feature_probs_1[key] = count_news/total\n",
    "            \n",
    "        ## Now finding probabilities for each word\n",
    "        self.feature_prob = {}\n",
    "        for key,value in vocabulary.items():\n",
    "            count = 0\n",
    "            for news_item in (news):\n",
    "                if news_item[value] != 0:\n",
    "                    count += 1\n",
    "            self.feature_prob[key] = count/len(news)\n",
    "            \n",
    "    def test(self, test_news):\n",
    "        \"\"\"\n",
    "        compute posterior probability\n",
    "        \"\"\"\n",
    "        target = []\n",
    "        for news in test_news:\n",
    "            prior_0 = 1\n",
    "            prior_1 = 1\n",
    "            evidence = 1\n",
    "            for i, val in enumerate(news):\n",
    "                key = list(vocabulary.keys())[i]\n",
    "                if key not in self.feature_prob.keys():\n",
    "                    continue\n",
    "                if val != 0:\n",
    "                    prior_0 *= self.feature_probs_0[key]\n",
    "                    prior_1 *= self.feature_probs_1[key]\n",
    "                    evidence *= self.feature_prob[key]\n",
    "\n",
    "            p_0 = (prior_0 * self.probs_0)/(evidence+1)\n",
    "            p_1 = (prior_1 * self.probs_1)/(evidence+1)\n",
    "\n",
    "            target.append(0 if p_0>p_1 else 1)\n",
    "        \n",
    "        return target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf988785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver code for training Naive Bayes\n",
    "\n",
    "nb = Gaussian_NB()\n",
    " \n",
    "nb.train(news_vecs,train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c77af3",
   "metadata": {},
   "source": [
    "#### test accuracy below is shown for only 50 test instances (news items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff1a5c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy:  56.00000000000001 %\n"
     ]
    }
   ],
   "source": [
    "## Driver code for testing Naive Bayes\n",
    "\n",
    "test = fetch_20newsgroups(subset = 'test',categories=categories)\n",
    "\n",
    "test_labels = test.target[:50]\n",
    "\n",
    "## Preprocess the test_data\n",
    "test_news_items = []\n",
    "for news in test.data[:50]:\n",
    "    test_news_items.append(preprocess_text(news))\n",
    "    \n",
    "test_news_vecs = []\n",
    "for news in test_news_items:\n",
    "    test_news_vecs.append(bag_of_words(news))\n",
    "\n",
    "preds = nb.test(test_news_vecs)\n",
    "\n",
    "\n",
    "test_acc = np.sum(preds==test_labels)/float(test_labels.shape[0]) \n",
    "\n",
    "print (\"Test Set Accuracy: \",test_acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5817a",
   "metadata": {},
   "source": [
    "#### Naive Bayes test accuracy on dataset size of 100 --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfcf0c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy:  60.0 %\n"
     ]
    }
   ],
   "source": [
    "## Driver code for testing Naive Bayes\n",
    "\n",
    "test = fetch_20newsgroups(subset = 'test',categories=categories)\n",
    "\n",
    "test_labels = test.target[:100]\n",
    "\n",
    "## Preprocess the test_data\n",
    "test_news_items = []\n",
    "for news in test.data[:100]:\n",
    "    test_news_items.append(preprocess_text(news))\n",
    "    \n",
    "test_news_vecs = []\n",
    "for news in test_news_items:\n",
    "    test_news_vecs.append(bag_of_words(news))\n",
    "\n",
    "preds = nb.test(test_news_vecs)\n",
    "\n",
    "\n",
    "test_acc = np.sum(preds==test_labels)/float(test_labels.shape[0]) \n",
    "\n",
    "print (\"Test Set Accuracy: \",test_acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712d692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
